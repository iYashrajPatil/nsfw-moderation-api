# ğŸ›¡ï¸ NSFW Image Moderation API (NudeNet + Rule-Based Scoring)

An **NSFW image moderation system** that classifies images as **SAFE**, **NSFW**, or **REVIEW** using **NudeNet** for detection and a **custom rule-based scoring layer** to reduce false positives.

This project focuses on **practical content moderation**, not just raw machine-learning output.

---

## ğŸ“Œ Problem Statement

Platforms that accept user-generated images (social media, forums, marketplaces) must automatically detect **explicit or inappropriate visual content** to ensure safety and compliance.

Pure ML-based detection systems often suffer from:
- âŒ High false positives (beach, gym, medical images)
- âŒ Lack of explainability
- âŒ Poor control over moderation strictness

---

## ğŸ§  Solution Overview

This project solves the problem using a **hybrid approach**:

### 1ï¸âƒ£ Detection Layer (Machine Learning)
- Uses **NudeNet** to detect exposed or covered body parts
- Returns labels with confidence scores

### 2ï¸âƒ£ Decision Layer (Rule-Based Logic)
- Applies domain-specific rules
- Aggregates confidence scores
- Produces a final moderation verdict

This ensures **better accuracy, transparency, and control**.

---

## ğŸ—ï¸ System Architecture

Image Input
â†“
NudeNet Detection
â†“
Label-wise Confidence Scores
â†“
Rule-based Aggregation
â†“
Final Verdict (SAFE / NSFW / REVIEW)


---

## ğŸ§ª Classification Logic

### ğŸ”´ NSFW (Explicit Content)
- Exposed genitalia or anus
- High-confidence exposed female breasts
- Cumulative explicit score exceeds threshold

### ğŸŸ¡ REVIEW (Ambiguous Content)
- Multiple soft signals detected
- Borderline confidence levels
- Requires human verification

### ğŸŸ¢ SAFE (Allowed Content)
- No detections
- Contextual or non-sexual exposure
- Scores below defined thresholds

---

## ğŸ“‚ Project Structure

NSFW-moderation/
â”‚
â”œâ”€â”€ App/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ classifier.py # Core NSFW detection & rule logic
â”‚ â”œâ”€â”€ main.py # FastAPI entry point
â”‚ â””â”€â”€ tmp_uploads/ # Temporary image storage
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ render.yaml
â””â”€â”€ README.md


âš ï¸ **Note:**  
Model files (`.onnx`) are intentionally **not committed** due to GitHub size limits.  
NudeNet automatically downloads required models at runtime.

---

## âš™ï¸ Core Detection Strategy

- **Explicit body parts** â†’ high weight
- **Soft body parts** â†’ cumulative weight
- **Threshold-based decision** to reduce false positives

This avoids cases like:
- Beach photos flagged as NSFW
- Fitness or medical images misclassified
- Single soft detection causing rejection

---

## ğŸ“¦ Example API Output

### NSFW Image
```json
```json
{
  "verdict": "NSFW"
}
```
### Safe Image
```json
{
  "verdict": "SAFE"
}
```
### Reviewed Image
```json
{
  "verdict": "REVIEW"
}
```

## ğŸš€ Key Features

âœ… Lazy model loading (memory efficient)

âœ… Rule-based false positive reduction

âœ… Clear moderation outcomes

âœ… FastAPI-based architecture

âœ… Easy to extend with new rules

## âš ï¸ Limitations

Scene-level context understanding is limited

Cultural interpretations of nudity may vary

Video moderation not included (image-only)

## ğŸ”§ Future Enhancements

Context-aware detection using CLIP / ViT

Human-in-the-loop moderation

Video frame analysis

Confidence calibration with real datasets

## ğŸ§‘â€ğŸ’» Tech Stack

Python 3

FastAPI

NudeNet

ONNX Runtime

Rule-based decision engine

## ğŸ“Œ Use Cases

Social media moderation

Marketplace image screening

Content safety pipelines

Automated pre-moderation systems

## ğŸ“œ Disclaimer

This project is intended for educational and research purposes.
Final moderation accuracy depends on model behavior and threshold configuration.

## â­ Final Note

This project demonstrates:

Practical ML integration

Engineering judgment

Explainable moderation decisions

Real-world content safety challenges